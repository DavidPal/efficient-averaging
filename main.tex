\documentclass{article}

\usepackage{fullpage,amsthm,amsmath,amssymb}
\usepackage{algorithm,algorithmic}

\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\nnz}{nnz}

\begin{document}

\title{Efficient Implementation of Averaging for Adaptive Online Learning}
\author{D\'avid P\'al}
\maketitle

This document describes efficient algorithm for averaging for online linear
optimization. The update takes $O(\nnz(g_t))$ time where $g_t$ is the gradient
vector in iteration and $\nnz(g_t)$ is \emph{number of non-zeros} in $g_t$.

We consider algorithms that process gradient vectors $g_1, g_2, \dots$ in
$\R^d$ in an online fashion and produce iterates $w_1, w_2, \dots$ in $\R^d$.
The iterate $w_t$ depends only on $g_1, g_2, \dots, g_{t-1}$.  We assume that
next iterate $w_t$ (or a constant multiple of it) can computed in
$O(\nnz(g_t))$ time from $g_t$ and internal state of the algorithm.

For stochastic optimization problems, one needs to produce running \emph{average}
$$
A_t = \frac{1}{t} \sum_{s=1}^t w_s
$$
of the iterates $w_1, w_2, \dots, w_t$. Point of this note to show
how produce the sequence $A_1, A_2, \dots$ such that the next average $A_t$
can be computed in amortized $O(\nnz(g_t))$ time.

Let
$$
S_t = \sum_{s=1}^t w_s
$$
be the sum. Then, $A_t = \frac{1}{t} S_t$.

\section{Assumptions on the algorithm}

We assume that
$$
w_{t+1} = \alpha_t \sum_{s=1}^t \beta_s g_s \; .
$$
where $\alpha_t$ and $\beta_t > 0$ can be computed in $O(\nnz(g_t))$ time
from $g_t$ and internal state of the algorithm.  Note that gradient descent
(AdaGrad) as well as FTRL (SOLO, Pistol, KT) are of this form.

To simplify the treatment, let $h_t = \beta_t g_t$. Then
$$
w_{t+1} = \alpha_t \sum_{s=1}^t h_t \; .
$$

This algorithm cannot explicitly compute $w_t$ in $O(\nnz(g_t))$ time.
However, it can produce a constant multiple of it in $O(\nnz(g_t))$ time.
More precisely, instead of working with $w_{t+1}$ we work with pair
$$
(\theta_t, \alpha_t)
$$
where
$$
\theta_t = \sum_{s=1}^t h_s \; .
$$
We can think of the pair $(\theta_t, \alpha_t)$ as a representation of $w_{t+1}$.
Clearly, $\theta_t$ can be computed from $\theta_{t-1}$ and $g_t$ in $\nnz(h_t)
= \nnz(g_t)$ time.

\section{Updating $S_{t+1}$ in amortized $O(\nnz(g_t))$ time}

We now show how to update $S_{t+1}$ in amortized $O(\nnz(g_t))$. In other
words, we show how to compute $S_T$ in time $O(\sum_{t=1}^T \nnz(g_t))$ time
and $O(d)$ memory. The basic idea is to postpone the updates and have a cache of
size $O(d)$ to store the updates. When the cache gets full, we realize the
updates.

In more detail, we remember $S_\tau$ and $w_\tau$ for some $\tau \le t$.
We can think of the variables $S_\tau$ and $w_\tau$ as the
latest versions. The cache contains the vectors $h_\tau, h_{\tau + 1},
\dots, h_t$ and the coefficients $\alpha_\tau, \alpha_{\tau + 1}, \dots,
\alpha_t$. The vectors $h_\tau, h_{\tau + 1}, \dots, h_t$ are stored as
sparse vectors. The cache takes
$$
\underbrace{(t - \tau)}_{\text{for storing $\alpha$'s}} + \underbrace{\sum_{s=\tau + 1}^t \nnz(h_s)}_{\text{for storing $h$'s}} \; .
$$

Whenever the space needed for cache exceeds  (some constant multiple of) $d$,
we compute $S_{t+1}$ and $w_{t+1}$ explicitly and we empty the cache. We now
show how to compute $S_{t+1}$ and $w_{t+1}$ in $O(d)$ time.

First note that $t \le \tau + d$, since the cache contains coefficients
$\alpha_\tau, \alpha_{\tau + 1}, \dots, \alpha_t$ even for zero gradients. Thus, the cache
reaches capacity $d$ in at most $d$ rounds. We can thus
explicitly compute partial sums $\gamma_{\tau}, \gamma_{\tau + 1}, \dots, \gamma_t$
where
$$
\gamma_s = \sum_{j=1}^s \alpha_j \; .
$$

Consider any cordinate $i \in \{1,2,\dots,d\}$. Let $s_1 < s_2 < \dots < s_k$
be the time indices such that $h_{s_j,i} \neq 0$ for $h_{s_j}$ in the cache.
We also define $s_0 = \tau - 1$ and $s_{k+1} = t$. (Note that $k=0$ can happen if
coordinate $i$ didn't receive any updates.) We have
\begin{align*}
S_{t+1,i}
& = S_{\tau,i} + \sum_{s=\tau + 1}^{t+1} w_{s,i} \\
& = S_{\tau,i} + \sum_{s=\tau}^t \alpha_s \theta_{s,i} \\
& = S_{\tau,i} + \sum_{j=0}^k \sum_{s=s_j + 1}^{s_{j+1}} \alpha_s \theta_{s,i} \\
& = S_{\tau,i} + \sum_{j=0}^k \theta_{s_j,i} \sum_{s=s_j + 1}^{s_{j+1}} \alpha_s \\
& = S_{\tau,i} + \sum_{j=0}^k \theta_{s_j,i} (\gamma_{s_{j+1}} - \gamma_{s_j}) \; .
\end{align*}
Clearly, the last sum can be evaluated in $O(k)$ time by computing $\theta_{s_0,i}, \theta_{s_1,i}, \dots, \theta_{s_k,i}$.
All coordinates of $S_{t+1}$ can be computed in $O(\sum_{i=1}^d k_i) =
O(\sum_{s=\tau + 1}^t \nnz(h_s)) = O(d)$ time.

\end{document}
